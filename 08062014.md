### Notes for Today

## Coursera Statistical Inference

* [**Week 4**]
	* [**P-values**](http://bcaffo.github.io/courses/06_StatisticalInference/03_03_pValues/#1)
		
		* Idea: Suppose nothing is going on -- how unusual is it to see the estimate we got?
			* Define the hypothetical distribution of the test statistic under null
			* Calculate the value of our test statistic at hand, based on observed data
			* Compared what we calculated to our hypothetical distribution and see how unusual it is

		* Formally, p-value is the probability (under the null) of obtaining evidence as extreme or more extreme than what we observed. low p-value means we will reject the null because it is extremely unlikely to happen if the null were to be true.

		* Another interpretation of p-value: It is the smallest value for alpha that you still reject the null hypothesis, also known as attained significance level. I don't exactly know how to use this definition though.

	* [**Power**](http://bcaffo.github.io/courses/06_StatisticalInference/03_04_Power/#1)
		
		* Power is the probability of **rejecting the null** hypothesis when it is **false**. We want power to be as big as possible.

		* A type II error is when we fail to reject the null when the null is false. It's often denote as \beta, and power = 1 - \beta.

		* **slide 8/12** is important: The equation yield in that slide gives us the relationship between power, \mu_a, \sigma, n, and \beta. Specifying any 3 of the unknowns and you can solver for the remainder. This is the essence of **power analysis**:
			* First decide on the requirement of your test: power, significance level
			* We then ask for the desired effect size (explained below)
			* Finally, we can figure out the required sample size in order to have a alpha level false positive + 1 - beta powerful test!

		* Power doesn't need \mu_a, \sigma, and n alone, instead only \sqrt(n) * (\mu_a - \mu_0) / \sigma. The quanitty \mu_a - \mu_0 / \sigma is called the effect size, the difference in the means in standard deviation units. Being unit free, it has some hope of interpretability across settings.

	* [**Multiple Testing**](http://bcaffo.github.io/courses/06_StatisticalInference/03_05_MultipleTesting/#1)
		* Hypothesis testing/signifance analysis is commonly overused, correcting for multiple testing avoids false positives or discoveries.

		* _slide 7/19_ is helpful to get an overall picture. 
			* **False positive rate** - the rate at which null results (\beta=0) are called significant
			* **Family wise error rate (FWER) ** - The probability of at least one false positive
			* **False discovery rate (FDR) ** - The rate at which claims of signifance are false

		* Controlling family-wise error rate (FWER):
			* Bonferroni correction, simply make alpha -> alpha / # tests run
			* Easy to calculate, but mgiht be too conservative

		* Controlling false discovery rate (FDR)
			* Order the p-values from smallest to largest (order statistics)
			* Cann any P_(i) <= alpha * i / m significant
			* Still pretty easy to calculate, less conservative. This means it will allow more false positives, but in exchange to have a higher chance of identifying when there is indeed a signal.
			* Might behave strangely under dependence.

		* Adjusted p-value are essentially the same as the two methods mentioned above, but we adjust the p-value instead of alpha.

