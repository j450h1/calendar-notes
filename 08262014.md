### Notes for Today

* **Coursera: Regression Models: Week 2, but up until simple linear regression**: Today's topic covers the statistical simple linear regression model, what it means to fit a model, how to evaluate the fit by looking at residuals. And finally, since we have a probablistic model, we can make inference on the regression coefficients. 

	* [**A statistical linear regression model**](http://bcaffo.github.io/courses/07_RegressionModels/01_05_linearRegression/#1)
		
		* Earlier, we were just finding the best fit line. There was no probability model involved. In this module, we model X and Y not only by a linear relationship, but with a noise term, where the noise term is probablistic (is gaussian distributed). This set up opens a lot of interesting discussions:
			
			* Y_i's are now N(mu, sigma^2) distributed too!
			* Because of this, the fitted coefficients also have their own distributions, this is what makes inference possible!
		
		* The optimization (fitting problem) now becomes a statistical problem, and we use MLE to find the maximum likelihood estimators. It turns out the least square solution is the MLE!

		* Interpreting regression coefficients:

			* (slide 5/14) \beta_0 is the expected value of the response when the predictor 0, not always interesting. But if we shift the X value by a, the slope doesn't change, but the intercept does. However, with this change, the interpretation of the intercept is much more understandable -- It becomes the expected value when X equals a. Often we choose a to be X_bar.

			* (slide 6/14) For \beta_1, the slope coefficient, it is the expected change in response for a 1 unit change in the predictor. Note that linear regression is cale invariant, so if you multiply x by a, then \beta_1 will become \beta_1 / a.

	* [**Residual and residual variation**](http://bcaffo.github.io/courses/07_RegressionModels/01_06_residualVariation/#1)

		* Remember the \epsilon part of the statistical model, those are the irreducible noise even if we know perfectly beta_0 and beta_1. In the data, we have e = y - y_hat, this is the vertical distance between the observed data point and the regression line. Least square minimize \sum e_i^2, the e's can be thought of as estimates of the \epislons.

		* Residuals are useful for investigating poor model fit. It can be thought of as the outcome with linear association of predictor X removed. It highlights poor model fit.

		* If an intercept is included, \sum e_i = 0 (I think it has to do with the linear algebra 4 fundamental subspaces, but I don't remember anymore). If a regression X is included, \sum e_i * X_i = 0.

		* The non-linear data (slide 7/17) and heteroskedasticity (9/17) are two great examples of how diagnostic graphs on residuals against X can be super-helpful!

		* Total variation = variation explained by the model + residual variation. Normalize it and we get R^2. In the univariate case, R^2 is literally the correlation(X,Y) squared.

		* R^2 can be misleading, so use it with caution.

	* [**Inference in Regression**](http://bcaffo.github.io/courses/07_RegressionModels/01_07_inference/#1)

		* Again, recall that we have a probablistic model, which brings life to Y (it's a random variable), and since beta's are linear combination of Y's, they are also random variable.

		* In layman's term, it means that when we have a new data set, we might get different Y's, and in terms, different MLE intercept and slope. How can we know if our betas are good with just one replica of the data.

		* The secret lies in statistical inference. Because we know the distribution of betas (under the null or alternative), we can perform hypothesis tests to understand / peak the true value of intercept and slope.

		* Omit the derivation here (see slide 5/14), and see slide 6/14 for re-implementing the simple linear regression estimates instead of lm.

		* Prediction: prediction interval are typically wider than confidence interval, and they are DIFFERENT. Here is the basic idea:
			* We are quite confident in the regression line (provided n is large), so that interval is rather narrow. If we knew the true beta_0 and beta_1, this interval would have zero width.
			* The prediction interval must incorporate the variability in the data arond the line. In other words, even if we knew \beta_0 and \beta_1, this interval would still have width because y flunctuates. so we have to deal with the (potential) inference uncertainty, but also the y uncertainty!
			

