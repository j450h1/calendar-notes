#### Cousera: Practical Machine Learning: Week 3

The materials from the third week is kind of messy -- it covers trees & generative algorithm. The introduction is pretty shallow and doesn't give a holistic introduction.

* Basic Decision Trees:
	* Pros: 
		* Easy to interpret
		* Better performance in nonlinear settings (Reminds me of the ISLR example and pictures)
		* It is good at modeling interactions between variables
	* Cons: 
		* Without pruning/cross-validation can lead to overfitting
		* Harder to estimate uncertainty (? not sure what this means)
		* Results may be variable

	* Building trees in R: 
		* Caret package can be used with rpart as well: `modFit <- train(Species ~., method = 'rpart', data = training)`
		* Take advantage of `library(rattle); fancyRpartPlot(modFit$finalMdodel)` to make pretty decision trees!

* Bagging (Bootstrap Aggregating):
	* Basic idea: Build on top of regular trees. It resample data and recalculate prediction. Average or majority vote -- this approach have similar bias, but dramatically reduce variance.
	* The simulation in slide 4/12 and 5/12 is a good example to show how bias didn't change much, but variance got reduced dramatically.

* Random Forest:
	* Basic idea: It's a special case of bagging, where for each bootstrap sample, the tree construction process only consider a random subset of predictors to split. (hence random forest)
	* Reminds me of the discussion of bias-variance-trade-off and how this further reduce the variance. I should revisit the tree model chapter in ISLR.

* Boosting:
	* Basic idea: Combining weak predictors into a strong predictors.
	* Basic idea 2: Everytime we parse through a weak classifier, we learned about what is it that is hard to be classified. We then give more weights to those and make sure we can do a better job classifying them next time.


* Model based prediction:
	* Basic idea: assume the data follow a probablistic model.
	* Pros:
		* can take advantage of structure of data, if exists
		* May be computationally more convenient
	* Cons:
		* make additional assumption
		* When the model is incorrect you may get reduced accuracy
	* Examples:
		* LDA, QDA, Naive Bayes

I AM NOT VERY SATISFIED WITH TODAY'S NOTES. THEY ARE VERY SHALLOW. Take a look at ISLR knowledge vault on trees and generative algorithms, and compare the depth.