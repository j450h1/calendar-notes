### Notes for Today

#### Coursera: Practical Machine Learning: Week 4

Again, the introduction here is very superficial, so I am not quite sure what notes to take here.

* Regularized Regression: Briefly talk about the idea of bias-variance-trade-off. Explain how by giving up a bit of bias, we are able to reduce variance significantly.

* Combining prediction functions:
	* Random forest, bagging, boosting: combining similar prediction functions
	* Model stacking: like the Netflix price: combining completely different models

* Forecasting: Not a bad read. It also mentioned Rob Hyndman's Forecasting: principles and practice, which I have read in the past

* Unsupervised prediction: Sometimes you don't know the labels for prediction, so to build a predictor, we can:
	* create clusters
	* name clusters
	* build predictor for clusters
	* in a new data set, predict clusters

---

#### Coursera: Mining Massive Data Set (MMS): Week 1 

Just the Map-reduce part. It seems like a good start, but I am still evaluating whether to go through with it or not.

* [Distributed File System]
	
	* Motivation: Single node architecture is limited by data bandwidth between CPU memory and disk. When the data set is huge, it will take forever to even read the data, let alone processing it.
	
	* Cluster computing try to distribute this work to many machines (Google has 1M+ machines) to parallelize the read and data processing oeprations.
	
	* Challenges of cluster computing:
		* A single server don't fail often, but a cluster of server fails quite frequently. 
			* How to store data persistently and keep it available if nodes can fail? 
			* How to deal with node failures during a long running computation?
		* Network bottleneck: moving very large data around is still slow (e.g. network bandwidth = 1 Gbps, moving 10TV takes approx 1 day)
		* Distributed programming is hard: need a simple model that hides most of the complexity
	
	* Map-Reduce solves the above problem:
		* Failure problem: store data redundantly
		* Network bottleneck: move computation to the data instead of data to the computation
		* Simple programming model: map and reduce
	
		* Redundant Storage Infrastructure: Data are broken up to chunks, and each chunk is replicated on different machines (nodes) and possibly different racks. This introduce concepts such as:
			* Chunk servers (slave nodes)
			* Master node
			* Client library for file access

* [Computational Model]

	* Slide 6 - 8 gives a nice graphical explanation of the paradigm.
	* word count example, url/host example, language model 5 word example.

* [Scheduling and Data flow]:

	* Slide 2 - 3 shows the physical representation of the map-reduce flow (Especially slide 3, IMPORTANT). Map-reduce is responsible for
		* **Paritioning** the input data into chunks and replicate them
		* **Scheduling** the program's execution across a set of machines
		* Performing the **group by key** step by using `hash(map.key) mod R` where R is the number of reducers.
		* Handling node **failures**
		* Manage inter-machine **communication**.

	* Data flow:
		* input and final output are stored on DFS, and scheduler tries to schedule task "close" to physical storage location of input data.
		* Intermediate results are stored on local FS because moving data around is costly
		* Output is often the input to another map-reduce task (DAG?)

	* Coordination: Master:
		* Master pgins workers periodically to detect failures, check task status (idle, in progress, completed), and idle tasks get scheduled as workers become available.

	* Dealing with failures:
		* When map phase failed -- completed and in-progress tasks are reset. The whole thing needs to be re-do for completeness.  
		* When reduce phase failed -- only in progress tasks are reset to idle, since the completed ones are good output that can be stored on HDFS standalone. 
		* Master node is replicated to avoid single point of failure.

	* How many Map and Reduce jobs: M map tasks, R reduce tasks
		* Rule of thumb: Make M much larger than the number of nodes in the cluster. Often DFS chunk per map is common
		* Usually R is smaller than M, because output is spread across R files.

* [Combiners and Partitioners]

	* Combiners takes place after map phase and before reduce phase. It aggregates outputs of the map tasks' key-value pairs. Can save network time by pre-aggregating values in the mapper!
	
	* Combiner trick works only if reduce function is communitive and associative (like sum, but not average [can be modified], and not median [all pairs of the same key need to be shipped to one reducer])
	
	* Parition function: control how keys get shipped to the reducers. Sometimes useful to override the default hashfunction (e.g. hash(hostname(url)) instead of hash(url)) 


[Distributed File System]: https://d396qusza40orc.cloudfront.net/mmds/lecture_slides/MapReduce1_DistributedFileSystems.pdf
[Computational Model]: https://d396qusza40orc.cloudfront.net/mmds/lecture_slides/MapReduce2_TheMapReduceComputationalModel.pdf
[Scheduling and Data flow]: https://d396qusza40orc.cloudfront.net/mmds/lecture_slides/MapReduce3_SchedulingAndDataFlow.pdf
[Combiners and Partitioners]: https://d396qusza40orc.cloudfront.net/mmds/lecture_slides/MapReduce4_CombinersAndPartitionFunctions.pdf