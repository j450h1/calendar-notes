### Notes for Today

## Coursera Exploratory Analysis Class

* **Week 3**:
	* [**Hierarchical Clustering**](https://d396qusza40orc.cloudfront.net/exdata/lecture_slides/hierachicalClustering.pdf):
		
		* Clustering is about "finding things that are close to together", questions like, how do we define close? how do we group things? are important
		* Hierarchical clustering is one clustering techniques. We will cover the _agglomerative approach_, where we try to merge two closest clusters together recursively. The methods requires two things:
			
			* A defined distance of similarity:
				* Continuous - Euclidean distance
				* Continuous - Correlation Study
				* Binary - Manhattan Distance
			
			* A merging approach (linkage)
				* Complete linkage: two clusters are measured by the farthest distance apart
				* Average linkage: two clusters are measured by the centroid distance
				* Single linkage: two clusters are measured by the closest distance apart
		
		* The clustering might be unstable due to:
			* change a few points
			* have different missing values
			* pick a different distance
			* change the merging strategy
			* change the scale of points for one variable

		* It's never obvious what is the right configuration, the trick is to try different configuration to find a stable clustering. _I still don't have a working knowledge of when to use what though!_.

		* The lecture also mentioned about _heatmap_ and _myplcust_ (slide 15/21):
			* _heatmap_: groups rows and columns and re-arrange them into blocks. Often good for visualizing matrices
			* _myplclust_: A function that allows us to plot pretty dendrograms, with clusters labeled.

	* [**K-means CLustering**](https://d396qusza40orc.cloudfront.net/exdata/lecture_slides/kmeansClustering.pdf)
		* Same questions? How do we define close? How do we group things?
		
		* A partioning approach:
			* Fix a number of clusters
			* Get "centroids" of each cluster
			* Assign things to closest centroid
			* Recalculate centroids
		
		* Requires:
			* A defined distance metric
			* A number of clusters
			* An initial guess as to cluster centroids

		* Produces:
			* Final estimate of cluster centroids
			* An assignment of each point to clusters

	* [**Dimension Reduction**](https://d396qusza40orc.cloudfront.net/exdata/lecture_slides/dimensionReduction.pdf)
		* Motivation, you have multivariate variables X_1 ... X_n:
			* Find a  new set of multivariate varaibles that are uncorrelated and explain as much variance as possible. (statistical)
			* If you put all the variables together in one matrix, find the best matrix created with fewer variables (low rank) that explains the original data. (data compression)

		* THe solution to these problems are PCA and SVD (It didn't go into the math)

		* Talks about variance explained + talk about the need to impute missing data before using SVD. It made the suggestion of using impute.knn in {inpute}.

		* Notes for this section is not great




