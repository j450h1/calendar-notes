### Notes for Today

Today I embarked on the journey of **Coursera: Practical Machine Learning**, the second to last class in the Data Science Specialization track. Upon watching videos from week 1, some revelations came to my mind:

Real learning do not occur if I just blindly or mechanically regurgitate what I know (at best I am just transcribing notes). New insights/chunk comes when:

* I learned a completely new concept (e.g. R profilers, R markdown)
* The materials that I understood was explained in a different context (e.g. multivariate regression coefficient is negative but univariate is positive - masking, or an existing concept that is demonstrated through simulation) and I was able to tie to a previous example I saw. This reinforces 'chunking'.

I already learned a lot of the fundamentals from ISLR, so most likely I already know a lot of the materials covered in this class. Therefore, to avoid regurgitating the materials, I should hold myself to a higher standard -- Do not go for completeness, rather, reinforce ideas when a known concept re-occur. 

Side note: The key to absorb the materials here is different from Coursera: Regression Model, where the 'meat' is in the simulation examples.

---

* The central dogam of prediction: It's not just about training a prediction function, the underlying probability sampling model is also important!

* Creating good features are heard: good features should lead to data compression, retain relevant information, and are often created based on expert application knowledge. 

* A good machine learning model weighs the trade-off of accuracy with:
	* Interpretability
	* Simplicity
	* Speed
	* Scalability

* In sample v.s. out of sample error. In sample error is capturing both signal and noise. Out of sample is a much better measure. This means one needs to have good model validation set up -- the typical setting is to break data into training, validation, and test set.
	* Training - use to build the model 
	* Validation - use to tune the model
	* Test - use to test the out of sample error

* Review sensitivity, specificity, accuracy. I would recommend going back to the ISLR notes again to internalize these.

* Bayes rule demonstrated again how prevelance of a disease can significantly change p(disease | positive test) - good demonstration.

* Common error measures:
	* MSE or RMSE: for continous data. But this measure is sensitive to outlier
	* Median absolute deviation -- continous data, more robust
	* Optimize sensitivity (recall) -- good if you want few missed positives
	* Optimize specificity -- good if you want few negative called positives. P(test negative | truth negative) high.
	* Accuracy -- weights false positives/negatives equally
	* Concordance -- not sure what this is

* ROC curve -- again, why is a random classifier gives the 45 degree line?

* The trade-off of cross validation, even k in the k-fold can be a tuning parameter and deals with bias-variance trade-off.
	* large k - less bias, more variance
	* small k - more bias, less variance
	* bootstrap underestimate the out of sample error

I should probably go back to ISLR to study again why these statements are true. I don't remember the details here.

...to be continued


