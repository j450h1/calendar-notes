### Notes for Today

Continue with Coursera: Data Science Track: Regression Model. Today, the class focus on multivariate regression interpretation, the usage of Dummy variables, and finally interaction variables.

* [**Interpreting the coefficients**](http://bcaffo.github.io/courses/07_RegressionModels/02_02_multivariateExamples/#1)
	
	* Using the example in slide 5/35, we see that the multivariate coefficient on _agriculture_ (-0.1721) has its sign reverse when comparing to just regressing response on agriculture alone `y ~ agriculture` (0.1942). Why might that be the case?

	* Slide 6/35 gives a good simulation example! 
		* Notice `x2 <- 1 : n;`, `x1 <- 0.01 * x2 + runif`, and `y = -x1 + x2 + rnorm`
		* The basic idea is that x2 was the real driver of correlation with y, but because x1 is correlated with x2 (`0.01 * x2`), so it can '**mask**'/inflate its correlation with y if that's all we regresson on. (i.e. when you do `y ~ x1`, the coefficient is a large positive number)
		* When you include both of them `y ~ x1 + x2`, we see that when controlling x2, the effect of x1 becomes quite small (and the sign reverse to -1, as the definition suggested)
		* slide 7/35 is a good visual illustration

	* This lesson is important, it means we need to be very careful in interpreting the univariate regression coefficients when the predictors are correlated!

	* What if we include an unneccesary variable `z = swiss$Agriculture + swiss$Education`, and `Fertility ~ . + z`. In fact, since z is redundant/linear combination of existing variables, z will get dropped!


* [**Dummy Variable**](http://bcaffo.github.io/courses/07_RegressionModels/02_02_multivariateExamples/#10)

	* When thinking about dummy variable, I like his approach, which is to think in terms of in expectation of Y for group x=1 and group x=0. see slide 10/35.

	* When there are multilevel factors, we will need a reference model, and the interpretation of the coefficient also changes (it's always referencing with the base level). see slide 11/35. He also illustrates how to do this manually in lm (slide 14/35) and how we should only include #factor - 1 terms (the base factor would be redundant if we include them, since they have to sum to 1)

	* If we omit the intercept `lm(count ~ spray -1)`, then each factor would have a coefficient (including base factor), and the interpretation of the factor is the average of the response in each level. (slide 16/35). But why???

	* [Summary](http://bcaffo.github.io/courses/07_RegressionModels/02_02_multivariateExamples/#17) in slide 17/35 summarizes well this section!


* [**Interaction Variable**](http://bcaffo.github.io/courses/07_RegressionModels/02_02_multivariateExamples/#1): I think Jeff Leek's data analysis on interaction does a better job, but let's review the main points again.

	* Fit two lines (one for male, one for female), each of them would have a separate intercept and coefficient

	* Two lines, same slope (different intercept, but same slope), see slide 29/35 for how it is model. Basically, it adds an additional dummy variable on the intercept term.

	* Two lines, different slope (different intercept, different slope), this use the interaction technique. see slide 31/35, where the interaction is on `b3 * I(sex = "male") * Y`. 
		* Again, interaction can be thought in terms of expectation in y (see slide 34/35).