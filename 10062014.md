#### Coursera: Measuring causal effect in the social science

I randomly ran into this course, and I think it's appealing to me because I want to learn more about the causal studies. Here is the course syllabus:

* Monday 6 October: Module 1 - The nature of causal effects and how to measure them 
* Monday 13 October: Module 2 - The multivariate regression model and mediating factors 
* Monday 20 October: Module 3 - Randomized controlled trials
* Monday 27 October: Module 4 - Instrumental variables 
* Monday 3 November: Module 5 - Difference in difference

The lecture videos are surprisingly short, perhaps 15 minutes per week. I think you might want to supplement that with background reading, based on this [book]. Keep an eye on this course, and keep evaluating whether it is good or not.

---

#### Doing Data Science

Today I embarked on the 3rd chapter of this book. So far, I really do enjoy the readings, they are easy to follow. Like one of the book reviews pointed out, this book is not as dry, since it covers a lot of informal, philosophical discussions. This is a good way to help me re-wiring the mental models. 


I am planning to do one chapter a week, and there might be weeks where I need to chuck this and read more ahead of myself since there are 11 chapters.

Some quick summary of chapter 3 - Algorithm:

Broadly speaking, there are three types of algorithm that is concerned in this book:

* Data processing algorithm - like sorting, map-reduce ... etc.
* Optimization algorithms - stochastic gradient descent, newton's method, least square and more.
* Machine learning algorithms and statistical learning models

Also broadly speaking, there are overlaps and differences between Statistics and ML:

* Interpreting parameters: Statisticians care more about interpretations
* Confidence interval: Interested in capturing the variability of the parameters
* Assmptions: statistical models can explicit assumptions. Non-parametric models are more flexible.

I really like how she talks about (pg.55) the key of a mature data scientist is not that they know the algorithms, but when to use what models under different circumstances. I can't agree more, and I think ISLR did a great job in pointing these out.

Three basic Algorithms - Linear regression | K nearest nighbors | K means:

* Linear regression:
	* Start with tidy data, and plot the relationship between one y and one x.
	* Given the data, we can find the best fit. Until this point, it's just curve fitting, no statistical model involved yet
	* Least square is the standard way of getting best fit
	* Beyond just curve fitting using least square:
		* Adding in modeling assumptions about the errors: this is where the assumption makes the whole exercise statistical -- By modeling the error, we suddenly can model y and its variability. (pg.65)
		* Adding in more predictors
		* Transforming the predictors 

Again, even linear regression makes a ton of assumptions, be aware of all these:
	* Linearity
	* Error terms normally distributed with mean 0
	* Error terms independent with each other
	* Error terms have constant variance indpendent of value of x (not heterosdastic)

Continue next time on k-nn & k-means


[book]: https://openknowledge.worldbank.org/bitstream/handle/10986/2693/520990PUB0EPI1101Official0Use0Only1.pdf?sequence=1
