### Notes for Today

## Coursera Data Science Series: "Getting and Cleaning Data"

* **(Week 4) Editing Text Variables**: [slide](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/04_01_editingTextVariables.pdf)
	* **toupper(), tolower(), strsplit()**
	* **sub() & gsub()**: sub() only replace one instance, gsub replace all instances in that string
	* **grep() & grepl()**: 
		*grep("Alameda", county)* will give us the indices for reocrd where country = alameda
		*grep("Alameda", country, value = TRUE)* gives the actual values instead of the indices
		*grepl("Alameda", country)* returns a vector of TRUE/FALSE
	* **library(stringr)**: is very handy. Function such as nchar, substr, paste, paste0 (no space), and str_trim
	* **Variable names should be**:
		* All lower case when possible
		* Descriptive (Diagnosis v.s. Dx)
		* Not duplicated
		* Not have underscores, dots, or white spaces

* **(Week 4) Regular Experssion I/II: [slide1](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/04_02_regularExpressions.pdf) & [slide2](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/04_03_regularExpressionsII.pdf). This is the kind of lecture that you really need to see examples! Not just a summary.
	* Regular Expression can be thought of as a combination of *literals* and *metacharacters*
		* Literals form the words of the language
		* Metacharacters define the grammar

	* Metacharacters:
		* **^Begin**: express the beginning of a sentence
		* **end$**: express the end of a sentence
		* **[]**: represent a character class. can also use [^char1char2...], this means include everything but char1 and char2 in the character class
		* **|**: translate to OR, and we can include any number of alternatives. e.g. alt1 | alt2 | .... | alt10
		* **()**: subexpressions are often contained in parentheses to constrain the alternative. e.g. ^([Gg]ood)|[Bb]ad). () can also be used to 'remember' text matched by the subexpression, and the matched subexpression can be referred by \1, \2...etc.
		* **?**: the question mark indicates that the indicated expression is optional
		* **.**: refers to any character
		* ** * **: means 'any number, including none, of the item'
		* **+**: means at least one of the item
		* **{ and }**: referred to as interval qualifiers; this let us specify the minimum and maximum number of matches of an expression

	* If we often need to express a literal character that is a metacharacter, we need to escape them with \. e.g. (\.)

* **(Week 4) Working with Dates**: [slide](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/04_04_workingWithDates.pdf)
	* **Start simple**: d1=date() or d2=Sys.Date()
	* **Formatting dates**: %d = day as number (0-31), %a = abbreviated weekday, %A = unabbreviated weekday, %m = month(00-12), %b = abbreviated month, %B = unabbreviated month, %y = 2 digit year, %Y = four digit year
	* **Creating dates**: e.g. x = c("1jan1960"); z = as.Date(x, "%d%b%Y")
	* **Getting parts of a date**: In the base package, can use weekdays(d2), month(d2), julian(d2) to get Sunday, January, X days since 1970-01-01
	* **Lubridate**: a very handy package that handles date objects. e.g. *ymd("20140108"), mdy("08/04/2013"), dmy...etc.* We can also deal with times using *ymd_hms*. 
	* In lubridate, to get the date part, the syntax is slightly different. e.g. wday to get the day of the week

## Recap notes from earlier weeks of Coursera "Getting and Cleaning Data"

* **Week 1**:
	* The goal of this course: Raw data -> Processing script -> Tidy Data -> Data analysis -> Data communication. We will focus on the first three steps, wheras traditional classes often only focus on the last two.
	* Describe components of getting to tidy data: The important thing is to have:
		* A code book describing each variable and its values in the tidy data set.
		* An explicit and exact recipe you used to go from raw data to tidy data, there is no parameter to the script.

	* [**Downloading files**](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/01_04_downLoadingFiles.pdf):
		* **setwd**: to set working directory
		* **file.exists & dir.create**: *if(!file.exists("data")) dir.create("data")* create a directory if it doesn't exist
		* **list.files()**: like the unix ls command
		* **download.file()**: with parameter fileUrl, destfile, method
	
	* [**Reading local files**](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/01_05_readingLocalFiles.pdf):
		* **read.table**: the default, flexible and robust. Reads the data into RAM - big data can cause problems.
		* **read.csv**: very similar to read.table, but the delimeter is assumed to be comma.
		* **Some more important parameters**: quote, na.strings, nrows, skip. In the instructor's experience, often quotation marks ' or " placed in data values cause troubles, setting quote="" resolves these.

	* [**Reading excel files**](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/01_06_readingExcelFiles.pdf):
		* **library(xlsx)**: can use read.xlsx to read .xlsx excel files
		* **read.xlsx**: with parameter *sheetIndex*, *colIndex*, *rowIndex*
		* **write.xlsx**: write into a excel file

	* [**Reading XML**](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/01_07_readingXML.pdf):
		* Extracting XML is the basis of web scraping, there are two components:
			* Markup - labels that give the text structure
			* Content - the actual text of the document
		* **Tags, elements, and attributes**: very similar to the same ideas in HTML
		* **Read an XML file into R**:
			* Use **xmlTreeParse** to load the doc, parameter useInternal=TRUE means we want to get all children nodes. Sometimes we would use **htmlTreeParse** to parse and load the doc.
			* Use **xmlRoot(doc)** to get the root note. At this point the whole doc is a huge list, and we can reference them using list syntax (or xmlSApply) or programmatically extract them (xpathSapply)
			* **XPath**: has its own syntax for extracting nodes that satisfy certain conditions
			* **xpathSapply**: can use to loop through specific node in the XML

	* [**Reading JSON**](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/01_08_readingJSON.pdf):
		* **library(jsonlite)**: use this pakcage to deal with JSON objects
		* **fromJSON**: to load JSON objects
		* **toJSON**: very handy if we need to write data frames to JSON
		* Check out the links in the end if you want to do more!

	* [**Data.table**]: We have already covered in 05312014.md

* **Week 2**:
	* [Reading from MySQL](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/02_01_readingMySQL.pdf)
		* **RMySQL package**: Each MySQL server can have multiple databases. Each database can have multiple tables. Each table can have multiple fields (columns) and records (rows). Each table is like a data.frame.
			* **dbConnect**: to establish a connection to a MySQL server *dbConnect(MySQL(), user, host, db)*
			* **dbGetQuery**: to send a query to the server/db *dbGetQuery(ucscDb, "show databases;")*
			* **dbListTables**: to list all the tables in a db
			* **dbListFields**: to get al the fields of a table
			* **dbReadTable**: reads in a db.TABLE as a data.frame
			* **dbSendQuery**: Different from dbGetQuery, it only store the query without executing it
			* **fetch**: Often use with dbSendQuery, we can fetch the results incrementally. when finished fetching the results, need to close it by **dbClearResult**.
			* **dbDisconnect**: when finished, need to close the connection to the server/db.
	* [Reading HDF5 data](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/02_02_readingHDF5.pdf)
		* Hierarchical data. check out the video to see more details
	* [Reading from the Web](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/02_03_readingFromTheWeb.pdf)
		* Webscraping: programmatically extracting data from the HTML code of websites
		* **readLines**: *readLines(url(http://scholar.google.com))* would return the raw HTML in raw text
		* **library(XML) & htmlTreeParse & xpathSApply**: Use XML package and htmlTreeParse to get the DOM, and use xpathSApply to parse the tree.
		* **library(httr)**: Very similar to using XML & htmlTreeParse & xpathSApply. 
		* **AUthentication**: *Get(url, autheticate("user","password"))*
	* [Reading from APIs](https://d396qusza40orc.cloudfront.net/getdata/lecture_slides/02_04_readingFromAPIs.pdf)
		* When there are APIs available from a serive, it's a much better option than webscraping. The basic idea is that you create an application with the service (say Twitter), and you autheticate yourself in a controlled environment so you can get access to data according to the rules of the service.
		* Typically, you want to create an application -> and get the key, secret, token, token secret
		* Use R to establish your credential so you can use the service.
			* **oauth_app**: with parameter key & secret
			* **sign_oauth1.0**: with parameter myapp, token, token_secret
			* **GET**: once the app verfies its credential, can use GET method to get data via API, the data is often returned in .json form.
			* **content & jsonlite::fromJSON**: once you got the .json object returned by the API service, you need to parse the json object to get the information you need.
		* **httr**: powerful library to interact with APIs
			* allows GET, POST, PUT, DELETE requests if you are authorized
			* You can authenticate with a user name or a password
			* Most modern APIs use something like Oauth
			* httr works well with Facebook, Google, Twitter, Github, etc.
	* Skip the Reading from other sources section.