#### Doing Data Science: Chapter 5: Logistic Regression

Since this book is more philosophical & practical, I would focus more not on the specific techniques, but some of the thinking/considerations that one needs to think through when 'doing data science'.

When building a classification model (or any model in general), you have to consider the following design choices:

* **Which classifier to use?**
	* In choosing the model, there are real constraints that you need to take into account:
		* Runtime: how long does it take to update the model? How long does it take to make the prediction on a new data point
		* You: Do you understand enough of the algorithm to make the necessary tweak to the algorithm?
		* Interpretability: How interpretable do you need to model to be?
		* Scalability: Learning time, scoring/prediction time, and model storage are all major concerns.

* **Which features to take from the data?**
	* This is the realm of feature engineering, and requires creativity. Check out the feature [engineering article] I discovered.
	* For this example, the feature are simply the urls a user visited, represented in binary form (1 if visited, 0 otherwise). For practical systems, the urls are often hashed.

* **Which loss function to minimize?**
	* For logistic regression, the basic idea is still based on the concept of Maximum likelihood.
	* The objective function is the maximum likelihood, where we model each user being independent. And each user (independently) decide whether they want to click on an ad or not (so bernoulli model).
	* Most importantly, we model that clicking probability is a function of the urls that the user visited.
	* Streaming everything together, we end up with the likelihood function.
	* Commonly, we minimize the negative log likelihood instead

* Which optimization method to employ?
	* Newton's method: leverage the curvature of the log-likelihood function. Inverting the Hessian matrix is very expensive when you have, say, 10,000 features (urls).
	* Stochastic Gradient Descent: Very popular in Mahout and VW, since it doesn't involved expensive matrix inversions.

* Which evaluation metric to use?
	* I really depends on the context, even if you are using the same model. e.g. Let's say you use logistic regression for ranking (by ranking the probability for each class) v.s. classification (0 or 1). What should be the right evaluation metric?
		* I don't think the book did a good job in explaining this. It said that for:
			* Ranking: use AUC, but it's not obvious to me
			* Classification: Can use precision, recall, F score and more.

---
#### Edx: Introduction to Linux

We talked about `cat, echo, sed, awk, sort, uniq, paste, join, split, regular expression, grep, tr, tee, wc, cut, less, head, tail, string, z command family`. 

![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/echo.png)
![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/sed1.png)
![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/sed2.png)
![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/awk.png)
![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/grep.png)
![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/tr.png)

[feature engineering]: http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/