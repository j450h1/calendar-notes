### Notes for Today

Today is about documenting my learnings from Coursera: Practical Machine Learning: Week 2 materials. Remember (and to emphasize again), the notes here are not meant to summarize the lectures, but to annotate some of the important learnings.

* The main focus of this week's material is on `Caret`, a R package designed to modularize common operations in predictive modeling, including but not limited to 

	* data preprocessing
	* split data
	* train/test model
	* model comparison (e.g. confusion matrix)

I never really used this package, and based on my learning from ISLR, I don't think mastery of this package is essential. But it might also be that I haven't built enough models to feel the pain point.

* [**Data spliting**]: Arguably, you can use `sample` everything that is done here, but it's still helpful to have helper functions:

	* `createDataPartition`: essentially the same as if we use `sample` - it splits the indices into training and test set.
	* `createFold`: this is super handy because we can store the indices for each of the k fold in a matrix, and the whole k-fold is stored in a list. 
		* returnTrain = TRUE will give the indices for training set, otherwise the indices for the test set is returned.
		* The trick is to use `train = data[folds[1],], test = data[-folds[1]]`
	* `createResample` creates bootstrap samples, and return the indices again.
	* `createTimeSlices` is also interesting since it creates this rolling windows of train/test that are time dependent. See the output to know the returned data structure.

* [**Plotting predictors**]: exploratory analysis is super important for generating hypotheses.

	* `featurePlot` in slide 5/14.
	* the `cut2` function in `Hmisc` package is super-helpful to breaking continous data into groups.
	* Aside: `grid.arrange` seems to arrange grid for ggplot2 figures

* [Preprocessing]: When should we pre-process:
	
	* When the data is very skewed. Box-Cox transforms or log(metric + 1) to look the distribution less skewed
	* When you want things to be one the same unit (standardization)
	* When there are missing data and you want to impute them (e.g. using k-nn would be an option)

* [Covariate Creation]: Often, this is two step process:

	* From the raw data, generate quantiative or qualitative covariate
	* Transform the covariate into better distribution. More necessary for linaer model than trees (more robust to normality assumptions)
	* Specifically, we can remove covariates with little variance, because most likely they are not going be differentiators. You can even include splines as a test feature.

I think this module brings an interesting question: Should we pre-process the test set exactly the same way as training set when making predictions? What is up with the example of standardization not using the test set, but only the training set.

* [**Train Options**]: The training phase is essentially built-in in the `train` functions. slide 3/10 demonstrate how many parameters you can set for your model. Notably, you can:

	* specify the model to run
	* Any pre-processing procedure
	* Give more weights to certain observation
	* What metric to optimize
	* controlling the training set

 Skipping the last three vidoes (PCA, regression).



[Data Spliting]: https://d396qusza40orc.cloudfront.net/predmachlearn/011dataSlicing.pdf
[Train Options]: https://d396qusza40orc.cloudfront.net/predmachlearn/012trainOptions.pdf
[Plotting predictors]: https://d396qusza40orc.cloudfront.net/predmachlearn/013plottingPredictors.pdf
[Preprocessing]: https://d396qusza40orc.cloudfront.net/predmachlearn/014basicPreprocessing.pdf
[Covariate Creation]: https://d396qusza40orc.cloudfront.net/predmachlearn/015covariateCreation.pdf