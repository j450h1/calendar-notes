### Notes for Today

## Coursera Statistical Inference Class
* **Week 3**:
	* [**Likelihood**](http://bcaffo.github.io/courses/06_StatisticalInference/02_04_Likeklihood/#1):
		
		* Likelihood of a collection of data is the joint density evaluated as a function of the parameters with the data fixed.
		* If X_{i} are independent random variables, then their likelihoods mutiply.
		* Ratios of lielihood values measure the relative evidence of one value of the unknown parameter to another. The ratio often can be use for hypothesis testing purposes.

	* **Example of Likelihood**:
		
		* A simple demonstration of likelihood using binomial distribution.
		* The number of heads/tails are sufficient to compute the likelihood (the concept of sufficiency)

	* **Maximum likelihood**:

		* MLE = the value of the parameter that would make the data that we observed most probable
		* Talked about another example of using Poisson distribution
		* One important way to see the likelihood is to plot it as a function of theta (the parameter), but normalized by the MLE. That way, you know the peak is at 1, and all the other points on the curve are basically relative importance to the MLE.
		* This set up is nice because you can start talking about 'likelihood interval' -- an interval that has height 1/8 means that no points in between has a 8 to 1 evidence -- All the points in between the likelihood interval has a stronger evidence that they are more likely to be the parameter compared to points outside the interval. (It's weird concept, I don't know how to use this)

	* [**Bayesian Analysis**](http://bcaffo.github.io/courses/06_StatisticalInference/02_05_Bayes/#1):
	
		* Posterior \prop Likelihood * Prior
		* Introduction to the beta density, used to model random varaible between 0 and 1, with parameter alpha and beta
		* Aside: the _manipulate_ function in R is quite cool, allows you to manipulate the plot you build by scroll bars!
		* Example: Posterior calculation of binomial(n,p) with p being beta distributed. 
		* The **posterior mean** is a mixture of the MLE (data) and the prior mean
		* The weight on data goes to 1 for large n. For small n, the prior mean dominates.
		* Generalizes how science should ideally work; as data becomes increasingly available, prior beliefs should matter less and less.
		* Credible interval is the bayesian analog of confidence interval. It has a particularly nice interpretation that the probability the parameter falls in this interval is 95% (much more natural interpretation than confidence interval).

	* [**Two group interval**](http://bcaffo.github.io/courses/06_StatisticalInference/03_01_TwoGroupIntervals/#1)

		* Classical Setting:
			* Compare two groups, the distribution of each group is i.i.d normal
			* We can the compute X_bar, Y_bar, S_x, S_y
			* Since both X and Y are normal, X_bar - Y_bar will be normal as well
			* When sample size of the two groups are drastically different, we might want to use the pooled variance. It's nothing but a convex combination of the group variances, placing greater weight on whichever has a larger sample size.
			* Check out the **putting this all together** page, it's the crucial page that explains why the ugly test statistics follows a t-distribution.

		* There is the *pair* case (didn't cover, but you need to fill in yourself). The idea is that if you have pair-measurement, the variance across pairs might be a lot smaller compared to the group variance.

		* *Another setting*: same as above, but unequal variance
			* The test statistics is different, but it's still t-distributed, with a really weird df, and a slightly different ways of calculating standard error.

		* *Other settings*: see slide 13/13. Binomial data, count data ... etc.

	* [**Introduction to Hypothesis Testing**](http://bcaffo.github.io/courses/06_StatisticalInference/03_02_HypothesisTesting/#1)

		* Decision framework based on data, will make type I or type II error. 
		* The convention is to control type I error, and balance the 'quality' of the test
		* slide 9/17 talks about when the test statistics should be compared with the critical point for 1 sided and 2 sided test.
		* There is a 1-1 connection of the confidence interval and rejection/acceptance of the test

