### Notes for Today

---

#### Continue on Coursera: Regression Model: Week 3: 3/3
To finish up what I didn't complete earlier this week. The last topic of week 3 materials is Mutliple variables. Some revelation: I don't think this course explains very well the concepts, but it does have the strength of explaining those concepts in **SIMULATION** or **R code**, I think that sort of hands on knowledge is important to develop!

* [Multiple Variable]
	
	* Modeling and Machine learning/Prediction have quite different objectives:
		* **Machine learning/Prediction**: Primarily looking for generalizability
		* **Modeling**: Primarily looking for interpretability and parsimonious representation.

	* General rules of inclusion/exclusion of regressors:
		
		* Omitting variables - results in bias in the coefficients of interest - unless their regressors are uncorrelated with the omitted ones.
			* Remember the simulated example `x2 = 1:n; x1 = 0.01*x2 + rnorm; y = -x1 + x2 + rnorm;`? When we only fit `y~x1`, it seems like `x1` is super important. But in reality, when x2 is included, the effect of x1 became negative.
			* I think ISLR has a similar example (check out "Woes of (interpreting) Linear/Logistic regression" and its [Credit Example]).
		
		* Including variables that we shouldn't have **increases** standard errors of existing regressors -- the concept of variance inflation factor (**VIF**). It also monotonically increase `R^2`. SIMULATION:
			* (Slide 6/14): compare coef of x1 before and after including x2 and x3, where x2 & x3 are uncorrelated with x1. Variance inflation is not severe.
			
			* (Slide 7/14): Same as above, but this time x2 and x3 are extremely correlated with x1. The variance inflation shoots up!
			
			* DEF: The variance inflation factor (VIF) is the increase in the variance for the regressor compared to the ideal setting where is is orthogonal to the other regressors.
			
			* VIF is only part of the picture, we do want to include certain variables, even if they dramatically inflate our variance.
				* **This statement actually confuses me, so how do we use VIF to determine whether to include/exclude a variable?** 
					* At the very least, I can know if one variable is severly correlated with another one using VIF.
				* You should review ISLR for this topic (I think it also explains the consequences on inference -> se goes up -> t-stat close to 0 -> H_0 that coef is 0 is not rejected. The consequence is that this decrease POWER -- the chance of us rejecting that coef is not 0 when it truly is not 0).

		* This omitting variable bias & including extraneous variable leads to bias-variance-trade-offs.

	* Covariate Model Selection

		* In machine learning class, will talk more about automated variable selection strategies, and PCA/factor analysis.

		* If the models of interest are nested and there are only a few regressors, then use nested likelihood ratio test (F-test) is a good approach. See slide 14/14 as an example using the `anova` function. 
			* I think the basic idea of that **F-test is to see if the redunction of the RSS is worth of adding additional variables**. You can think of it as making those additional covaraite with coef 0, and do F-test.

		* It's also a good idea to use covariate adjustment and multiple models to see if the treatment effect is being knocked out by certain combination of covariates.


[Multiple Variable]: http://bcaffo.github.io/courses/07_RegressionModels/02_05_multipleVariables/#1

---


