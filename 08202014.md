### Notes for Today

* [**Regression Models**](https://class.coursera.org/regmods-004/)

	* [**Week 1**](https://class.coursera.org/regmods-004/wiki/Lecture_note_links): prediction, build parsimonious model, investigate residual, look at fit coefficients, revisit assumptions, regression to the mean are all fair games.
		
		* **Basics**: 
			* Do you know Francis Galton (inventor of term regression) is actually the cousin of Charles Darwin!
			* We can use _library(manipulate)_ to see what mu minimize the sum of the square deviation, we can use it to vary the regression line. It's super handy to make an ordinary plot interactive!
			* Talk about regression through the origin, in R, it's as simple as _lm(I(child - mean(child)) ~ I(parent - mean(parent)) - 1, data = galton)_. The I syntax cast it as that exact expression, the -1 in the end get rid of the intercept.

		* **Notation and Derivation for Simple linear regression:**
			* Empirical mean (related to centering) 
			* empirical variance/sd (related to scaling)
			* normalization is centering + scaling, it tells us how many sd is the datum away from the mean.
			* covariance, correlation, measures linear association

		* **Least Squares estimation of simple regression lines:**
			* Instead of using calculus, it uses a trick to find the necessary condition to lead to the optimality condition (see notes 4/19).
			* Consider the case of **fitting only horizontal lines**, so beta_1 = 0, we want to fit for beta_0. The solution is beta_0 = Y_bar. (see slide 6/19 for derivation)
			* Consider **regression through the origin**, so beta_0 = 0, we want to fit for beta_1. The solution is <x,y> / <x,x>.
			* More general, *simple linear regression case*. We fit both beta_0 and beta_1.
				* beta_0 = Y_bar - \beta^{hat}_1 * X_bar (intercept)
				* **beta_1 = Cor(Y,X) * SD(Y) / SD(X)** (slope)
				* the fitted line pass through the average of X and Y.
			* If fit X as outcome and Y as predictor, slope is COR(Y,X) * SD(X) / SD(Y)
			* Slope is the same one if you center the data, and fit a regression through the origin
			* If normalize the data, the slope is COR(Y,X) * 1/1 = COR(Y,X)
			* See slide 13/19 - 16/19, demonstrate that the math is the same as the _lm_ result in R!\
		
		* **Regression down to Mediocrity**:
			* P(Y < x | X = x) gets bigger as x heads into a very large value
			* P(Y > x | X = x) gets bigger as x heads to very small values
			* When X & Y are normalized, the slope is COR(Y,X), regardless which variable is the outcome variable
				* If you had to predict a son's normalized height, it would be COR(Y,X) * X
				* If you had to predict a father's normalized height, it would be COR(Y,X) * Y
				* In both cases, multiplication by this correlation shrinks towars 0 (shrinks to the mean!)
				* If correlation is 1, then no regression to the mean!
			* If X is the outcome variable and you create a plot where X is the horizontal axis, the slope of least square is then 1/Cor(Y,X) (derivation?)
				* My reasoning: if X is place on vertical axis, then slope would be Cor(Y,X), now we reflect the image, so it becomes 1/COR(Y,X)
