### Notes for Today

Continue to explore Coursera: Data Science Track: Reproducible Research Week 3. I've watched the videos up until week4: caching computations. I can either end this module here or finish the last two case studies.

* [**Reproducible Research Checklist**](https://d396qusza40orc.cloudfront.net/repdata/slides/Checklist.pdf)
	
	* **DO: Start with good science**
		* Garbage in, garbage out principle
		* Coherent, focused question simplifies many problems
		* Working with good collaborators reinforces good practices
		* Something that's interesting to you will (hopefully) motivate good habits


	* **DON'T: Do things by hands**:
		* Editing spreadsheet of data to "clean it up"
		* Editing tables or figures (rounding, formatting)
		* Downloading data from a website (click links in a web browser)
		* Moving data around your computer; splitting/reformatting data files
		* "We're just going to do it once..."


	* **DON'T: Point And Click**: 
		* Many data processing /statistical analysis packages have graphical user interface (GUIs)
		* GUIs are convenient/intuitive but the actions you take with a GUI can be difficult for others to reproduce
		* In general, be careful with data analysis software that is highly interactive; ease of use can sometimes lead to non-reproducible analyses.


	* **DO: Teach a computer**:
		* If something needs to be done as part of your analysis, try to teach your computer to do it (program it). Teaching a computer almost guarantees reproducibility.


	* **DO: Use some version control**:
		* Slow things down
		* Add changes in small chunks (don't just do one massive commit)
		* Track/tag snapshot; revert to old versions
		* Use software like github/bitbucket/ sourceForge make it easy to publish results


	* **DO: Keep track of your software environment**:
		* Document your computer architecture, operating system, software toolchain, supporting software/infrastructure, external dependencies, version numbers.
		* In R, can type `sessionInfo()`


	* **Don't: Save Outputs**:
		* Avoid saving data analysis output (tables, figures, summaries, processed data), except perhaps temporarily for efficiency purposes.
		* Save the data+code that generate the output, rather than the output itself


	* **DO: Set your seed**: using `set.seed()`


	* **DO: Think about the entire pipeline**:
		* Data analysis is a lengthy process; it is not just tables/figures/reports
		* Raw data -> processed data -> analysis -> report
		* How you got the end is just as important as the end itself
		* The more of the data analysis pipeline you can make reporducible, the better for everyone
		* Ask yourself: How far back in the analysis pipeline can we go before our results are no longer (automatically) reproducible

* [**Evidence Based Data Analysis**](https://d396qusza40orc.cloudfront.net/repdata/slides/EvidenceBasedDataAnalysis.pdf)
	* Replication focuses on the validty, of the scientific claim. It is ultimate standard of science. New investigators, data ,analytical methods, laboratories, instruments should verify and obtain the same results

	* Reproducibility focuses on the validity of the _data analysis_. "Can we trust this analysis" is the central question. It's important to note that even a reproducible analysis can be invalid - just because we can reproduce the analysis doesn't mean the analysis itself is correct

	* See slide 10/46 for a high level picture of what reproducible data pipeline looks like

	* Talk about the problem with reproducibility:
		* how it address the most "downstream" aspect of scientific dissemination process. Advocacy of moving reproducible research to peer review stage, might be unrealistic.
		* Who should perform reproducible research:
			* general public: I don't care
			* scientisit who believe your claim, or the alternative
			* Others who just want to prove your claim is wrong

	* Talk about this concept of Deterministic Statistical Machine

* He also talked about the `cacher` R package, I think the concept is pretty interesting. It basically cache your R code into a SHA1, and someone who wants to perform reproducible research can do so:
	* Data objects are cached, but we store references to these data objects.
	* Upon requests, these data objects will be lazily evaluated and surface to the reproducible researcher
	* Expression that does not result in data objects will need to be executed again
	* We can get a DAG of dependency graph of how data objects in that analysis is related
	* Calling the object will display the particular code chunk that created the data object
	