#### Doing Data Science: Chapter 4

I like the way they think about what model to use for Spam classification:

* Linear Regression?
	* For spam classification, the outcome is binary (0 or 1). Linear regression doesn't give us that. That's a non-starter.
	* We can go ahead with it, and might found that the number of examples (n) is much smaller than the number of features (m), especially when we use each token as a feature. This might cause trouble when inverting the matrix.

* K-nearest neighbors?
	* Calculating the distance of a 10,000 dimensional vector could be costly, but that's not the main problem.
	* It suffers from a different problem -- the curse of dimensionality. When the dimension is 10,000, even the closest point would be far apart from each other.

* Naive Bayes is a better option!
	* It's a special case of generative models
	* The idea is quite simple, it uses bayes rule, and all we need to do is to estimate
		* p(spam) & p(ham), we called these priors
		* p(w|spam) & p(w|ham), these can be estimated directly from the data corpus
	* Keep in mind that these estimations has theorectical grounding via maximum likelihood (i.e. bernoulli -- do I see the word or not, given the class?)
	* This is theoretical under-pinning is briefly covered in the 'laplace smoothing section'
		* regular estimate - use MLE
		* laplace smoothing - use MAP (Maximum a posteriori likelihood)
	* The shell-script approach to model building is refreshing!
	