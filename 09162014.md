### Notes for Today

Vertica is a powerful tool that I learned on the job, but I never had a good chance to dive in and learn a little bit more. Today, I found out about Vertica's [web based training], which roughly covers the following areas:

* [The building blocks of Vertica]:
	* **Columnar Orientation**: Data is stored in columns instead of rows. Using only columns referenced in the query, you reduce memory usage and network I/O.
	* **High Availability**: Automatic redundancy and recovering ensures your ability to function, even when there are non-functional nodes.
	* **Advanced Compression**: Operating on encoded or compressed data saves capital expenses and data footprint.
	* **Automatic database design**: The database designer allows you to optimize, tune, and control your database. You can desing your schema without shutting down the database.
	* **Application integration**: Using standard SQL, allows interactions with an eco-system of other products.
	* **Massively Parallel Processing**: Using peer-to-peer nodes, each node process a subset of data, where the results are then combined in the end.

* [Installing Vertica and the Management Console] & [Creating a database] & [Loading data]: I'll skip these for now, since I haven't run into a case where I need to be a DBA (database administrator) myself. 
 
* [Introduction to Projection]
	* Vertica stores data in columnar format (unlike row-store databases), called proejctions. Vertica reads only the columns referenced by the query. Resources aren't wasted reading large amount of unused columns, optimizing the efficient query performance.
	* Three types of projections:
		* Super-projections: Created upon data loading.
		* Query-specific projections: Only load the necessary columns for operations and store them in adjacent nodes for fast querying.
		* Buddy projections: are created for fault tolerant purposes.
	* For projections that contain a large amount of data, Vertica will randomly and evenly divide the data in the projection and distribute the data subsets across all nodes in the cluster. It keeps the load on any one node to a minimum, and keeps the query against those data efficient.
	* If there is a small amount of data in the projections, Vertica might replicate the data onto each node.
	* When creating a projection, the `segmented by hash(column)` simply random hash the data based on that specific column.

* [Query Execution]
	* Execution process:
		* user submits a query
		* the query is submit to any nodes, the selected node is called initiator node. The choice of initiator node might be done by the load balancer.
		* the initiator node determines the best execution plan.
		* the execution plan is distributed to all other nodes, called executor nodes.
		* each node processed its subset of data, to get the partital results
		* the partial resutls then are return to the initiator node and aggregated, and the aggregated results is return to the client.
	* Use `explain` to learn the execution plan for a query.
	

* [Introduction to Analytics]: The heart of this web based training. Learn about Analytics function, and how they are different from the traditional SQL aggregation functions. I would just read the tutorial again.

* [Event-based Analytics]: still need to read

* [Timeseries Analytics]: still need to read

---
#### Other resources:
* [Understanding the Github flow]
* [What are some of software and skills that every data scientist should know]: It got a lot of answers recently

[web based training]: http://www.vertica.com/customer-experience/wbt/
[Understanding the Github flow]: https://guides.github.com/introduction/flow/index.html
[What are some of software and skills that every data scientist should know]: http://www.quora.com/What-are-some-software-and-skills-that-every-Data-Scientist-should-know
