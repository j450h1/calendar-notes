### Notes for Today

Vertica is a powerful tool that I learned on the job, but I never had a good chance to dive in and learn a little bit more. Today, I found out about Vertica's [web based training], which roughly covers the following areas:

* [The building blocks of Vertica]:

	* **Columnar Orientation**: Data is stored in columns instead of rows. Using only columns referenced in the query, you reduce memory usage and network I/O.
	* **High Availability**: Automatic redundancy and recovering ensures your ability to function, even when there are non-functional nodes.
	* **Advanced Compression**: Operating on encoded or compressed data saves capital expenses and data footprint.
	* **Automatic database design**: The database designer allows you to optimize, tune, and control your database. You can desing your schema without shutting down the database.
	* **Application integration**: Using standard SQL, allows interactions with an eco-system of other products.
	* **Massively Parallel Processing**: Using peer-to-peer nodes, each node process a subset of data, where the results are then combined in the end.

* [Installing Vertica and the Management Console] & [Creating a database] & [Loading data]: I'll skip these for now, since I haven't run into a case where I need to be a DBA (database administrator) myself. 
 
* [Introduction to Projection]

	* Vertica stores data in columnar format (unlike row-store databases), called proejctions. Vertica reads only the columns referenced by the query. Resources aren't wasted reading large amount of unused columns, optimizing the efficient query performance.

	* Three types of projections:
		* Super-projections: Created upon data loading.
		* Query-specific projections: Only load the necessary columns for operations and store them in adjacent nodes for fast querying.
		* Buddy projections: are created for fault tolerant purposes.
	
	* For projections that contain a large amount of data, Vertica will randomly and evenly divide the data in the projection and distribute the data subsets across all nodes in the cluster. It keeps the load on any one node to a minimum, and keeps the query against those data efficient.
	
	* If there is a small amount of data in the projections, Vertica might replicate the data onto each node.
	
	* When creating a projection, the `segmented by hash(column)` simply random hash the data based on that specific column.

* [Query Execution]

	* Execution process:
		* user submits a query
		* the query is submit to any nodes, the selected node is called initiator node. The choice of initiator node might be chosen by the load balancer.
		* the initiator node determines the best execution plan.
		* the execution plan is distributed to all other nodes, called executor nodes.
		* each node processed its subset of data, to get the partital results
		* the partial resutls then are return to the initiator node and aggregated, and the aggregated results is return to the client.
	* Use `explain` to learn the execution plan for a query.
	

* [Introduction to Analytics]: I think this is the heart of this web based training. Learn about Analytics function, and how they are different from the traditional SQL aggregation functions. I would just read the tutorial again.

	* First of all, you need to know that analytics functions are different from SQL's typical aggregation functions. ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/analytics_vs_aggregate.png)

	* It's also very importnat to know the analytics query structure. ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/analytics_query_structure.png)

	* With the basic `analytics_function() over (parition clause order by clause)`, you need to know the concept of window frame. ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/frame_clause.png). ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/frame_clause_2.png)

	* And finally, here are some great examples to reinforce the concept of frame. ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/frame_examples.png)

* [Event-based Analytics]: Mainly, we have three awesome analytics functions

	* CONDITIONAL_CHANGE_EVENT(): increment the counter if current row is different from the immediate previous row
	* CONDITIONAL_TRUE_EVENT(): evaulate the statement within (), and increment the counter if evaluated to TRUE.
	* LAG(): Using LAG() + CONDITIONAL_TRUE_EVENT(), we can create session_ids for each user. ![alt text](https://github.com/robert8138/Calendar_Notes/blob/master/images/sessionization.png)

* [Timeseries Analytics]: Use to interporlate time series data for missing timestamps

	* TIMESERIES(): Used to generate the missing data rows
	* TS_FIRST_VALUE() & TS_LAST_VALUE(): Used to interpolate the time series numbers (either constant or linear)

---
#### Other resources:
* [Understanding the Github flow]
* [What are some of software and skills that every data scientist should know]: It got a lot of answers recently
* [How to start a startup]: Y Combinator's Stanford class on how to start a startup

[web based training]: http://www.vertica.com/customer-experience/wbt/
[Understanding the Github flow]: https://guides.github.com/introduction/flow/index.html
[What are some of software and skills that every data scientist should know]: http://www.quora.com/What-are-some-software-and-skills-that-every-Data-Scientist-should-know
[How to start a startup]: http://startupclass.samaltman.com/
